---
title: "Node.jsã§å®Ÿç¾ã™ã‚‹LLMã®æ–°ã—ã„å¯èƒ½æ€§ - llama2ã‚’TypeScriptã§å‘¼ã³å‡ºã™æ–¹æ³• - "
emoji: "ğŸ¦™"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [llm, llama, nodejs, typescript]
published: true
---

# ã¯ã˜ã‚ã«

å…ˆæ—¥ã€Python ç’°å¢ƒã§ meta ãŒä½œã£ã¦å…¬é–‹ã—ã¦ã‚‹ LLM ãƒ¢ãƒ‡ãƒ«ã® llama2 ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã—ã¦ã¿ãŸ
TypeScript ã§ã‚‚ã§ãã‚‹ã®ã‹ãªã¨æ€ã£ã¦èª¿ã¹ã¦ã¿ãŸ
èª¿ã¹ã‚‹ã¨ã€node-llama-cpp ã£ã¦ã®ã‚’ä½œã£ã¦ã‚‹äººãŒã„ãŸ
llama.cpp ã£ã¦ã€llama ã‚’ C++ã§å‹•ãã‚ˆã†ã«ã—ãŸã‚„ã¤ã‚’ Node.js ã‹ã‚‰å‘¼ã¹ã‚‹ã‚ˆã†ã«ã—ãŸã‚‚ã®
https://github.com/withcatai/node-llama-cpp

ã•ã‚‰ã«ã€langchain.js ã‹ã‚‰ã‚‚å‘¼ã³å‡ºã›ã‚‹ã¿ãŸã„
https://js.langchain.com/docs/integrations/llms/llama_cpp
https://js.langchain.com/docs/integrations/chat/llama_cpp

# llama.cpp

ã¾ãšã€llama.cpp ã¯ Apple silicon ã§ä½¿ãˆã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã¦ã€Apple ã‚·ãƒªã‚³ãƒ³ã® GPU ã‚’ä½¿ã†ãŸã‚ã® Apple ç‰ˆ CUDA ã® Metal ã«å¯¾å¿œã—ã¦ãã‚Œã¦ã„ã‚‹ã€‚ã™ã”ã„ã€‚
`Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks`

https://github.com/ggerganov/llama.cpp

Metal éƒ¨åˆ†ã‚’ä½¿ã£ã¦ã‚‹ã‚³ãƒ¼ãƒ‰ã¯ã“ã“ã£ã½ã„ã‘ã©ã€ã“ã‚Œä»¥ä¸Šçªãé€²ã‚€æ°—åŠ›ã¯ãªã‹ã£ãŸã€‚
ã¡ãªã¿ã«ã€ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚‚ã‚ã‚‹ GGML ã£ã¦ã®ãŒ LLM ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚‰ã—ã„ã€‚ãŸã ã€llama.cpp ã¯ GGML ã˜ã‚ƒãªãã¦ GGUF ä½¿ã†ã‚ˆã†ã«ãªã£ãŸã¿ãŸã„ã€‚
GGUF ã¯ã€GGML Universal Format ã‚‰ã—ã„ã€‚
https://github.com/ggerganov/llama.cpp/blob/master/ggml-metal.m

ãƒ¢ãƒ‡ãƒ«ã¯ llama æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ãŒãŸãã•ã‚“ã‚ã£ã¦ã€ãã‚Œã¯ huggingface ä¸Šã§ãŸãã•ã‚“æä¾›ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€æ°—ã«ãªã‚‹ã‚‚ã®ã‚’å–å¾—ã—ãŸã‚‰ ok
https://huggingface.co/TheBloke

# node-llama-cpp è©¦ã—ã¦ã¿ãŸ

ç°¡å˜ã ã£ãŸ
node-llama-cpp ãŒã€ãƒ—ãƒªãƒ“ãƒ«ãƒ‰ã•ã‚ŒãŸãƒã‚¤ãƒŠãƒªã‚‚æä¾›ã—ã¦ãã‚Œã¦ã„ã‚‹ã¿ãŸã„
ãƒ“ãƒ«ãƒ‰ã™ã‚‹æ–¹æ³•ã‚‚æä¾›ã—ã¦ãã‚Œã¦ã„ã‚‹ã®ã§æ¥½ã¡ã‚“

ã“ã‚“ãªæ„Ÿã˜ã§ã€`node-llama-cpp`ã¯ä½¿ãˆã‚‹ã€‚ã“ã“ã§ã¯ã€ELYZA ã®æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã¿ãŸã€‚

```ts
import { fileURLToPath } from "url";
import path from "path";
import { LlamaModel, LlamaContext, LlamaChatSession } from "node-llama-cpp";

const __dirname = path.dirname(fileURLToPath(import.meta.url));

const model = new LlamaModel({
  modelPath: path.join(
    __dirname,
    "models",
    "ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf"
  ),
});
const context = new LlamaContext({ model });
const session = new LlamaChatSession({ context });

const q1 = "å…ƒæ°—ï¼Ÿ";
console.log("User: " + q1);
const a1 = await session.prompt(q1);
console.log("AI: " + a1);
```

# langchain è©¦ã—ã¦ã¿ãŸ

ã“ã£ã¡ã‚‚ç°¡å˜

```ts
import { LlamaCpp } from "@langchain/community/llms/llama_cpp";

const llamaPath =
  "/Users/hoge/hello-node-llama-cpp/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf";

const model = new LlamaCpp({ modelPath: llamaPath });

const question = "ãƒ©ãƒã£ã¦ä½•ï¼Ÿ";
console.log(`You: ${question}`);
const response = await model.invoke(question);
console.log(`AI : ${response}`);
```

Chat model ã‚‚ã“ã‚“ãªæ„Ÿã˜ã§ stream å‡ºåŠ›ã§ãã¦æ¥½ã¡ã‚“ã€‚

```ts
import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";

const llamaPath =
  "/Users/hoge/hello-node-llama-cpp/models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf";

const model = new ChatLlamaCpp({ modelPath: llamaPath, temperature: 0.7 });

const stream = await model.stream("ãƒ©ãƒã£ã¦ä½•ï¼Ÿ");

for await (const chunk of stream) {
  console.log(chunk.content);
}
```

# ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã¦ã¿ãŸ

node-llama-cpp ã§ã€ã©ã†ã‚„ã£ã¦ã€llama.cpp ã‚’å‘¼ã³å‡ºã—ã¦ã„ã‚‹ã‹æ°—ã«ãªã£ãŸã®ã§ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã¦ã¿ãŸ
ã©ã†ã‚‚ã€`llama/addon.cpp`ã§ [Node-API](https://nodejs.org/api/n-api.html) ã£ã¦ã®ã‚’ä½¿ã£ã¦å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã¿ãŸã„ã€‚
https://github.com/withcatai/node-llama-cpp/blob/master/llama/addon.cpp

ãƒ“ãƒ«ãƒ‰ã—ãŸã‚‚ã®ã¯ require ã§å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«ãªã‚‹ã¿ãŸã„
https://github.com/withcatai/node-llama-cpp/blob/master/src/utils/getBin.ts

langchain.js ã®æ–¹ã¯å˜ã« node-llama-cpp ã‚’å‘¼ã³å‡ºã—ã¦ã„ã‚‹ã ã‘ã¿ãŸã„
https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/llms/llama_cpp.ts

ã‚µã‚¯ãƒƒã¨ã—ã‹æ›¸ã„ã¦ãªã„ã‘ã©ã€ã‚ˆãã‚ã‹ã‚‰ãªã‹ã£ãŸã®ã§ã€chatgpt ã«èããªãŒã‚‰èª­ã‚“ã§ãŸï¼ˆãã“ã¯ llama ã«èã‘ã‚ˆï¼‰

# è£œè¶³

llama.cpp ã¯ä»–ã®è¨€èªã§ã‚‚å‘¼ã³å‡ºã›ã‚‹ã‚ˆã†ã«è‰²ã€…å–ã‚Šçµ„ã¿ãŒã‚ã‚‹ã¿ãŸã„ã€‚ã™ã”ã„ã€‚
https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#description

è‡ªåˆ†ã® PC ã§å‹•ã‹ã—ãŸã„ã ã‘ãªã‚‰ã€Ollama ãŒã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ã„ã„æ„Ÿã˜ã«ç”Ÿæˆã§ãã‚‹ã—ã€API ã‚µãƒ¼ãƒãƒ¼ã£ã½ãã‚‚å‹•ãã®ã§ãŠã™ã™ã‚
https://github.com/ollama/ollama

# ãŠã‚ã‚Šã«

- llama ã™ã”ã„
- llama.cpp ã™ã”ã„
- Node-API ã™ã”ã„
- ç”Ÿæˆ AI ã¯ åˆ¥ã« Python ã˜ã‚ƒãªãã¦ã‚‚ã„ã‚“ã˜ã‚ƒã­ï¼Ÿ
