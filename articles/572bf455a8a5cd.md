---
title: "Google Colabでllama.cpp"
emoji: "🦙"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [llama, llm, llamacpp]
published: true
---

# はじめに

Google Colab の無料版の GPU どれくらい使えるのかなと思って、llama を動かしてみた。

これをみながら試してみた。以下、ほぼ二番煎じなので、npaka さんのサイト色々みてもらう方がためになるかも。
https://note.com/npaka/n/n280ffc0d5ff0

ただ、.bin のファイルフォーマットは古そうな気がしたので、.gguf のフォーマットのファイルを使うことにした。なので、そこだけ違う。

# 環境

こんな感じ。メモリそこそこある。すごい。
![](/images/2024-02-18-19-49-57.png)
![](/images/2024-02-18-19-50-22.png)

# 試したこと

これを選んでみた。
https://huggingface.co/TheBloke/Vicuna-7B-CoT-GGUF

最初にダウンロード

```
!wget https://huggingface.co/TheBloke/Vicuna-7B-CoT-GGUF/resolve/main/vicuna-7b-cot.Q4_K_M.gguf
```

次に llama.cpp をビルド

```
!git clone https://github.com/ggerganov/llama.cpp
%cd llama.cpp
!mkdir build
%cd build
!cmake .. -DLLAMA_CUBLAS=ON
!cmake --build . --config Release
!cp bin/main ..
%cd ..
```

最後に実行

```
!./main -m ../vicuna-7b-cot.Q4_K_M.gguf --temp 0.1 -p "User:日本語で回答してください。富士山の高さは？ Assistant:" -ngl 32 -b 512
```

結果はこんな感じ。よかった。できた。時間もそんなにかからなかった。どれくらい時間かかってたか、ちゃんとみてなかったけど、数秒だったはず。

```
User:日本語で回答してください。富士山の高さは？ Assistant: 富士山の高さは、3776メートルです。 [end of text]
```

# メモ

色々間違えながらやってたんだけど、違うモデルでやろうとした時、out of memory とかも出てた。

```
ggml_backend_cuda_buffer_type_alloc_buffer: allocating 15780.62 MiB on device 0: cudaMalloc failed: out of memory
llama_model_load: error loading model: failed to allocate buffer
llama_load_model_from_file: failed to load model
llama_init_from_gpt_params: error: failed to load model '../japanese-stablelm-base-beta-70b.Q4_K_M.gguf'
main: error: unable to load model
```

# おわりに

しばらくしてから、実行だけしようとしたら、ファイル消えてたりもあった。モデルのダウンロードに時間がかかるから、ローカルでやる方がストレスないけど、気軽に GPU 使えるのすごい。
