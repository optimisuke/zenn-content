---
title: "Apple Silicon (M1, M2, M3) Mac ã§ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®LLaVAã‚’å‹•ã‹ã™æ–¹æ³•4ã¤"
emoji: "ğŸ¦™"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [llama, llava, mac, llm]
published: true
---

# ã¯ã˜ã‚ã«

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã® llama ã‚’ç”»åƒã‚‚å…¥åŠ›ã§ãã‚‹ã‚ˆã†ã«ã—ãŸ LLaVA ã‚’ M1 Mac ã§å‹•ã‹ã—ã¦ã¿ã¾ã—ãŸã€‚ä¸€éƒ¨å‹•ã„ã¦ã„ãªã„ã§ã™ãŒã€‚ã€‚ã€‚
ã„ã‚ã‚“ãªæ–¹æ³•ãŒã‚ã‚‹ã®ã§ã€æ•´ç†ã—ã¦ã¿ã¾ã™ã€‚

Llava ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„æ–¹ã¯ä¸‹è¨˜ã‚µã‚¤ãƒˆã‚’è¦‹ã¦ã¿ã‚‹ã®ãŒè‰¯ã„ã¨æ€ã„ã¾ã™ã€‚
https://llava-vl.github.io/

ä¸‹è¨˜ã‚µã‚¤ãƒˆã§å®Ÿéš›ã«è©¦ã›ã‚‹ã®ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã‹ã—ãŸããªã‚‹ã‚±ãƒ¼ã‚¹ã¯å°‘ãªã„ã‹ã‚‚ã§ã™ãŒã€‚ã€‚ã€‚
https://llava.hliu.cc/

ã¾ãŸã€Replicate ã¨ã„ã† Hugging Face ã«ä¼¼ãŸã‚µãƒ¼ãƒ“ã‚¹ã§ã€åˆ¶é™ã¤ãã§ã™ãŒç„¡å„Ÿã§ API ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ã‚µã‚¯ãƒƒã¨ API ã§å‘¼ã³å‡ºã—ãŸã„æ–¹ã¯ã“ã¡ã‚‰ã€‚
Python ã¨ã‹ Node.js ã® SDK ã‚‚ã‚ã£ã¦ã‚µã‚¯ãƒƒã¨å‘¼ã³å‡ºã›ã¾ã™ã€‚
https://replicate.com/yorickvp/llava-13b

# LLaVA

ã¾ãšã¯ã€github ä¸Šã«ã‚ã‚‹ã€ã“ã‚Œã‚’å‹•ã‹ãã†ã¨ã—ã¦ã¿ã¾ã—ãŸã€‚ã§ã‚‚ã€å‹•ã‹ãªã„ãƒ»ãƒ»ãƒ»ã€‚å‹•ã„ãŸäººã„ã‚Œã°æƒ…å ±ãã ã•ã„ã€‚

https://github.com/haotian-liu/LLaVA
https://github.com/haotian-liu/LLaVA/blob/main/docs/macOS.md

ã‚„ã£ãŸã®ã¯ã“ã“ã‚‰ã§ã™ã€‚

```
pyenv install 3.10
pyenv local
python -m venv .venv
source .venv/bin/activate
python -mpip install --upgrade pip  # enable PEP 660 support
pip install -e .
pip install torch==2.1.0 torchvision==0.16.0
pip uninstall bitsandbytes
python -m llava.serve.controller --host 0.0.0.0 --port 10000
python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload
python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit --device mps
```

ã“ã‚“ãªã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã—ãŸã€‚

```bash
% python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit --device mps
2023-11-29 22:28:40 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1.5-13b', model_base=None, model_name=None, device='mps', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=True)
2023-11-29 22:28:40 | INFO | model_worker | Loading the model llava-v1.5-13b on worker d9bfa3 ...
2023-11-29 22:28:40 | ERROR | stderr | Traceback (most recent call last):
2023-11-29 22:28:40 | ERROR | stderr |   File "/Users/ito/.pyenv/versions/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main

```

PyTorch ãŒ Mac ã«å¯¾å¿œã—ã¦ãŸã‚Šã™ã‚‹ã®ã§ã€å‹•ãæ°—ã¯ã—ã¦ã‚‹ã‘ã©ã€ã‚ˆãã‚ã‹ã‚“ãªã„ã§ã™ã€‚ã€‚ã€‚
è«¦ã‚ã¦ã€æ¬¡ã€‚

## å‚è€ƒ

https://note.com/ngc_shj/n/n2425276cacb6
https://pytorch.org/

# llama-cpp

llama ã‚’ C++ã§å‹•ã‹ã™ã‚„ã¤ã€‚
MacBook ã§å‹•ã‹ã™ã®ãƒ¡ã‚¤ãƒ³ã‚´ãƒ¼ãƒ«ã‚‰ã—ã„ã§ã™ã€‚

```
The main goal of llama.cpp is to run the LLaMA model using 4-bit integer quantization on a MacBook
```

https://github.com/ggerganov/llama.cpp

`Makefile`ã«è‰²ã€…æ›¸ã‹ã‚Œã¦ã‚‹ã®ã§ã€`make`ã™ã‚‹ã ã‘ã§ãã‚Œã£ã½ãå‹•ãã¾ã™ã€‚ã™ã”ã„ã€‚Apple Silicon ç‰ˆ CUDA ã® Metal ã‚‚ä½¿ã†ã‚ˆã†ã«ã—ã¦ãã‚Œã¦ã„ã¦ã€GPU ã‚’ä½¿ã£ã¦å‹•ã„ã¦ãã‚Œã¦ã¾ã™ã€‚ã™ã”ã„ã€‚

```
make
```

å‹•ã‹ã™å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã—ã¦ãã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ãƒˆãƒ©ãƒƒãƒ—ãŒã‚ã£ã¦ã€`.bin`ã®ãƒ¢ãƒ‡ãƒ« (GGML) ã ã¨å‹•ã‹ãªãã¦ã€`.gguf` (GGUF)ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã€‚2023-08 ã‚ãŸã‚Šã«ã€ç ´å£Šçš„å¤‰æ›´ãŒã‚ã£ã¦ã€ãã†ãªã£ãŸã¿ãŸã„ã€‚
æ¤œç´¢ã—ã¦å‡ºã¦ãã‚‹è¨˜äº‹ãŒã€å¤ã„è¨˜äº‹ãŒå¤šãã¦è¦æ³¨æ„ã€‚

GGUF ã®ãƒ¢ãƒ‡ãƒ«ã¯ Hugging Face ä¸Šã«è‰²ã€…ã‚ã‚Šã¾ã™ã€‚
TheBloke, mmnga ã£ã¦ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æ–¹ã€…ãŒè‰²ã€…å…¬é–‹ã—ã¦ãã‚Œã¦ã‚‹ã®ã§ã€ãã‚Œã‚’ä½¿ã£ãŸã‚‰è‰¯ã•ãã†ã§ã™ã€‚è‡ªåˆ†ã§ã€ã‚³ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã‚‚ã‚ã‚‹ã¿ãŸã„ã§ã™ãŒã€è©¦ã—ã¦ãªã„ã§ã™ã€‚å·¨äººã®è‚©ã«ç«‹ã£ãŸã‚‰ãˆãˆã‚„ã‚“ï¼Ÿ
https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML
https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf

ãŸã¨ãˆã°ã€ä¸Šè¨˜ã‚µã‚¤ãƒˆã‹ã‚‰`ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf`ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã¦ä¸‹è¨˜ã®ã‚ˆã†ãªã‚³ãƒãƒ³ãƒ‰ã§è©¦ã›ã¾ã™ã€‚

```
./main -m ./models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf --temp 0.1 -p "[INST]ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å‹‰å¼·ã™ã‚‹éš›ã€ã©ã®è¨€èªãŒã„ã„ã§ã—ã‚‡ã†ã‹ã€‚ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚[/INST]"
```

è‚å¿ƒã® llava ã¯ã€ã¾ãšä¸‹è¨˜ã‚µã‚¤ãƒˆã‹ã‚‰ã€`ggml-model-q4_k.gguf`ã¨`mmproj-model-f16.gguf`ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

https://huggingface.co/mys/ggml_llava-v1.5-7b/tree/main

ãã®å¾Œã€`models`ãƒ•ã‚©ãƒ«ãƒ€ã«ãŠã„ã¦ã€ä¸‹è¨˜ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚tokyo.jpeg ã¯é©å½“ã«æ‹¾ã£ãŸç”»åƒã‚’åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãŠã„ã¦è©¦ã—ã¾ã—ãŸã€‚

```
./llava-cli -m models/ggml-model-q4_k.gguf --mmproj models/mmproj-model-f16.gguf --image tokyo.jpeg -p 'ã“ã‚Œã¯ä½•ï¼Ÿæ—¥æœ¬èªã§ç­”ãˆã¦'
```

ã„ã„æ„Ÿã˜ã«å‹•ã„ã¦æ„Ÿå‹•ã€‚å…ˆäººãŸã¡ã«æ„Ÿè¬ã€‚

## å‚è€ƒ

https://zenn.dev/michy/articles/c3116fa7bc2e0b
https://zenn.dev/michy/articles/d13d24e5f19c56
https://qiita.com/rairaii/items/1c5c322f66b1423a74a0
https://note.com/bakushu/n/nbe8d2813c76b
https://nowokay.hatenablog.com/entry/2023/10/17/153657

# llama-cpp-python

æ¬¡ã¯ã€llama.cpp ã‚’ python å‘ã‘ã«ãƒ©ãƒƒãƒ—ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚ã“ã¡ã‚‰ã‚‚ã™ã‚“ãªã‚Šå‹•ãã¾ã—ãŸã€‚ç´ æ™´ã‚‰ã—ã„ã€‚
https://github.com/abetlen/llama-cpp-python

æº–å‚™ã¯ã“ã‚Œãã‚‰ã„ã€‚

```
pip install llama-cpp-python
```

ã¾ãšã¯ã€llama ã‚’å‹•ã‹ã—ã¦ã¿ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ã€llama-cpp ã®æ™‚ã¨åŒã˜ã‚ˆã†ã«ã€äº‹å‰ã« Hugging Face ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ã„ã¾ã™ã€‚

```python:main.cpp
from llama_cpp import Llama
llm = Llama(model_path="../llama.cpp/models/llama-2-7b-chat.Q4_K_M.gguf")
output = llm(
  "Q: Name the planets in the solar system? A: ", # Prompt
  max_tokens=32, # Generate up to 32 tokens
  stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
  echo=True # Echo the prompt back in the output
)
print(output)
```

å®Ÿè¡Œã¯ã“ã‚“ãªæ„Ÿã˜ã€‚

```
python main.cpp
```

llava ã¯ã“ã‚“ãªæ„Ÿã˜ã€‚URL æŒ‡å®šã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã€ãƒ­ãƒ¼ã‚«ãƒ«ç”»åƒã‚’ä½¿ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚

```python
from llama_cpp import Llama
from llama_cpp.llama_chat_format import Llava15ChatHandler
### Test
clip_model_path = "../llama.cpp/models/mmproj-model-f16.gguf"
model_path = "../llama.cpp/models/ggml-model-q4_k.gguf"

chat_handler = Llava15ChatHandler(clip_model_path=clip_model_path)
llm = Llama(
    model_path = model_path,
    chat_format = "llava-1-5",
    chat_handler = chat_handler,
    n_ctx = 2048, # n_ctx should be increased to accomodate the image embedding
    n_gpu_layers = 1,
    logits_all = True,
    verbose = False
)

output = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "ã‚ãªãŸã¯å®Œç’§ã«ç”»åƒã‚’æ—¥æœ¬èªã§èª¬æ˜ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": "https://zounokuni.com/wp/wp-content/themes/animal/images/animals/animals_list_15@2x.png"}},
                {"type": "text", "text": "ã“ã®ç”»åƒã‚’æ—¥æœ¬èªã§è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
            ]
        }
    ]
)
print(output)
```

```python
from llama_cpp import Llama
from llama_cpp.llama_chat_format import Llava15ChatHandler
import base64

def image_to_base64_data_uri(file_path):
    with open(file_path, "rb") as img_file:
        base64_data = base64.b64encode(img_file.read()).decode('utf-8')
        return f"data:image/png;base64,{base64_data}"


file_path = '../llama.cpp/tokyo.jpeg'
data_uri = image_to_base64_data_uri(file_path)

### Test
clip_model_path = "../llama.cpp/models/mmproj-model-f16.gguf"
model_path = "../llama.cpp/models/ggml-model-q4_k.gguf"

chat_handler = Llava15ChatHandler(clip_model_path=clip_model_path)
llm = Llama(
    model_path = model_path,
    chat_format = "llava-1-5",
    chat_handler = chat_handler,
    n_ctx = 2048, # n_ctx should be increased to accomodate the image embedding
    n_gpu_layers = 1,
    logits_all = True,
    verbose = False
)

output = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "ã‚ãªãŸã¯å®Œç’§ã«ç”»åƒã‚’æ—¥æœ¬èªã§èª¬æ˜ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": data_uri}},
                {"type": "text", "text": "ã“ã®ç”»åƒã‚’æ—¥æœ¬èªã§è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
            ]
        }
    ]
)
print(output)
```

ã©ã£ã¡ã‚‚ã„ã„æ„Ÿã˜ã§ã™ã€‚

# Ollama

https://ollama.ai/
ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‰ã™ã llama ã‚’ä½¿ãˆã‚‹ã‚„ã¤ã§ã™ã€‚ãƒ­ã‚´ãŒå¯æ„›ã„ã€‚
ã¾ã ã€llava ä½¿ãˆãªã„ã‘ã©ã€ã‚¤ã‚·ãƒ¥ãƒ¼ã‚‚ PR ã‚‚å‡ºã¦ã‚‹ã®ã§ãã®ã†ã¡ä½¿ãˆãã†ã€‚ç´ æ™´ã‚‰ã—ã„ã€‚
https://github.com/jmorganca/ollama/issues/746
https://github.com/jmorganca/ollama/pull/1216

# ãŠã‚ã‚Šã«

ã–ã£ã¨ã€è©¦ã—ãŸã‚Šèª¿ã¹ãŸã“ã¨ã‚’æ•´ç†ã—ã¦ã¿ã¾ã—ãŸã€‚
MacBook ã§å‹•ã‹ã™ãªã‚‰ã€llama.cpp ã¨ llama-cpp-python ã§ã„ã„ã‹ã¨ã„ã†æ°—æŒã¡ã«ãªã£ã¦ã¾ã™ã€‚Apple Silicon ã® GPU ã‚µãƒãƒ¼ãƒˆãŒã„ã¤ã®é–“ã«ã‹å……å®Ÿã—ã¦ã‚‹ã‚“ã ãªãã¨æ€ã„ã¾ã—ãŸã€‚
