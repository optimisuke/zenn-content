---
title: "Vercel AI SDK で Ollama を使う方法"
emoji: "🦙"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [vercel, llm, ollama, llama, react]
published: true
---

# はじめに

Vercel AI SDK (React 等から LLM の API をいい感じに stream で呼び出せるようにするやつ) から Ollama （OSS の LLM をローカルで動かすやつ） を呼び出す方法を調べました。

## 参考

https://zenn.dev/optimisuke/articles/c29fe9ef8cd28d

# 課題

Vercel AI SDK の サンプルコードを、OpenAI から Ollama の langchain のモデルを使って、置き換えて動かそうとしたけど、なぜかうまくいかなかった。

# 解決方法

ここのディスカッションにいろんな解決方法が記載されている。その中からいくつか試した。

https://github.com/vercel/ai/discussions/539

## 解決方法 1 OpenAI Compatibility API を使う

OpenAI API と同じ API で呼び出す方法。呼び出せるモデルに制約がある。マルチモーダルの llava は呼び出せない。
URL 変えるくらい。シンプル。すんなり動いた。
https://ollama.com/blog/openai-compatibility

## 解決方法 2 langchain の ChatOllama を使う

この人のコードを真似した。シンプルだしいい感じ。
これだと、修正したらマルチモーダルの llava も動いた。
https://github.com/brunnolou/next-ollama-app

## 解決方法 3 langchain の ChatOllama を使う（修正版）

比較的最近、langchain のライブラリの一部が、`@langchain/core`と`@langchain/community`にわかれた。
それに合わせると解決方法 2 はこんな感じ。

```ts:app/api/chat/route.ts
import { StreamingTextResponse, Message } from "ai";
import { AIMessage, HumanMessage } from "@langchain/core/messages";
import { ChatOllama } from "@langchain/community/chat_models/ollama";
import { BytesOutputParser } from "@langchain/core/output_parsers";

export const runtime = "edge";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const model = new ChatOllama({
    baseUrl: process.env.OLLAMA_BASE_URL,
    model: "mistral",
  });

  const parser = new BytesOutputParser();

  const stream = await model
    .pipe(parser)
    .stream(
      (messages as Message[]).map((m) =>
        m.role == "user"
          ? new HumanMessage(m.content)
          : new AIMessage(m.content)
      )
    );

  return new StreamingTextResponse(stream);
}
```

https://github.com/langchain-ai/langchainjs?tab=readme-ov-file#-what-is-langchain

## 解決方法 4 Vercel の ModelFusion を使う

ModelFusion（AI アプリを作るための TypeScript ライブラリ）を使う。
Vercel 大好きなら、これも良いかも。モデルに制約ありそう。

https://github.com/lgrammel/modelfusion-ollama-nextjs-starter

https://github.com/vercel/modelfusion

# おわりに

無事動いてよかった。
