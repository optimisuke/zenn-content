---
title: "LangChain/OpenAI SDK ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’OpenTelemetryã§Langfuseã«é€ä¿¡ã—ã¦ã¿ãŸ"
emoji: "ğŸ”­"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [langfuse, langchain, openai, observability, opentelemetry]
published: true
---

## ã¯ã˜ã‚ã«

Langfuse ã‚’ Docker Compose ã§ãƒ­ãƒ¼ã‚«ãƒ«èµ·å‹•ã—ã€LangChain/OpenAI SDK ã‚’ä½¿ã£ãŸ Python ã‚³ãƒ¼ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ OTLP (OpenTelemetry Protocol) é€ä¿¡ã™ã‚‹ã¾ã§ã‚’ã¾ã¨ã‚ãŸè¨˜äº‹ã§ã™ã€‚

ã‚³ãƒ¼ãƒ‰ï¼š

https://github.com/optimisuke/hello-langfuse-otel

## 1. å…¨ä½“åƒã¨æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³

- Langfuse (langfuse-web / langfuse-worker / Postgres / Redis / MinIO / ClickHouse) ã‚’å…¬å¼ docker-compose ãƒ™ãƒ¼ã‚¹ã§èµ·å‹•ã€‚
- Python ã‚¢ãƒ—ãƒªã¯ 4 ã¤ã®æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒˆãƒ¬ãƒ¼ã‚¹é€ä¿¡æ–¹æ³•ã‚’æ¯”è¼ƒã€‚
  - app1: LangChain ã® LLM ãƒ¢ãƒ‡ãƒ«ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ¸¡ã™ï¼ˆåŸºæœ¬å½¢ï¼‰
  - app2: LangChain ãƒã‚§ãƒ¼ãƒ³ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ¸¡ã™
  - app3: OpenAI SDK + Traceloop (OpenLLMetry) ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ OTLP é€ä¿¡
  - app4: LangChain ãƒã‚§ãƒ¼ãƒ³ + Traceloop (OpenLLMetry) ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ OTLP é€ä¿¡
- app3/4 ã¯ Traceloop.init ã§ OTLP ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ Basic èªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ Langfuse ã«è¨­å®šã—ã€OTel (OpenTelemetry)ã®ç’°å¢ƒå¤‰æ•°ã ã‘ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é£›ã°ã™æœ€å°æ§‹æˆã€‚

## 2. ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆ

```mermaid
flowchart LR
  subgraph Langfuse Stack
    LFWeb[langfuse-web:3000]
    LFWorker[langfuse-worker]
    CH[ClickHouse]
    PG[Postgres]
    RD[Redis]
    MINIO[MinIO]
    LFWeb --> LFWorker
    LFWorker --> CH
    LFWorker --> PG
    LFWorker --> RD
    LFWorker --> MINIO
  end

  subgraph Apps
    A1["app1<br/>LangChain + callback"]
    A2["app2<br/>LangChain + Chain + callback"]
    A3["app3<br/>OpenAI + Traceloop<br/>(OpenLLMetry)"]
    A4["app4<br/>LangChain chain + Traceloop<br/>(OpenLLMetry)"]
  end

  A1 -. Langfuse SDK .-> LFWeb
  A2 -. Langfuse SDK .-> LFWeb
  A3 -- OTLP --> LFWeb
  A4 -- OTLP --> LFWeb
```

## 3. å„ã‚¢ãƒ—ãƒªæ§‹æˆ

```mermaid
flowchart LR
  subgraph app1
    M1["ChatOpenAI<br/>callbacks=[LangfuseHandler]"]
    H1["Langfuse CallbackHandler"]
  end
  H1 -.callbacks on model invoke.-> M1
  M1 -->|invoke| LF[(Langfuse)]
```

```mermaid
flowchart LR
  subgraph app2
    H2["Langfuse CallbackHandler"]
    C2["chain2<br/>(Prompt1 â†’ Model â†’ Prompt2 â†’ Model)"]
  end
  H2 -.callbacks on chain invoke.-> C2 -->|invoke| LF[(Langfuse)]
```

```mermaid
flowchart LR
  subgraph app3
    TL3["Traceloop.init"]
    OA3["OpenAI Chat API"]
  end
  TL3 -.once at startup.-> OA3 -->|OTLP export| LF[(Langfuse)]
```

```mermaid
flowchart LR
  subgraph app4
    TL4["Traceloop.init"]
    C4["LangChain chain<br/>(Prompt1 â†’ Model â†’ Prompt2 â†’ Model)"]
  end
  TL4 -.once at startup.-> C4 -->|OTLP export| LF[(Langfuse)]
```

## 4. ã‚³ã‚¢ã‚³ãƒ¼ãƒ‰æŠœç²‹

### app1: ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

```python
handler = CallbackHandler(public_key=..., secret_key=..., host=...)
model = ChatOpenAI(model="gpt-4o-mini", callbacks=[handler])
ai_message = model.invoke(messages, config={"run_name": "chat-demo"})
handler.flush()
```

### app2: ãƒã‚§ãƒ¼ãƒ³ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

```python
handler = CallbackHandler(...)
chain1 = prompt1 | model | StrOutputParser()
chain2 = {"city": chain1, "language": itemgetter("language")} | prompt2 | model | StrOutputParser()
config = {"run_name": "chain2-two-step", "callbacks": [handler]}
answer = chain2.invoke({"person": person, "language": "æ—¥æœ¬èª"}, config=config)
handler.flush()
```

### app3: Traceloop (OpenLLMetry) + OpenAI

```python
auth = base64.b64encode(f"{pk}:{sk}".encode()).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = f"{base}/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {auth}"
Traceloop.init(app_name="app3-openllmetry", disable_batch=True,
               api_endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"),
               headers={"Authorization": f"Basic {auth}"})
resp = OpenAI().chat.completions.create(
    messages=[{"role": "user", "content": "LLM Observabilityã£ã¦ä½•ï¼Ÿ"}],
    model="gpt-4o-mini",
)
```

### app4: Traceloop (OpenLLMetry) + LangChain ãƒã‚§ãƒ¼ãƒ³

```python
auth = base64.b64encode(f"{pk}:{sk}".encode()).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = f"{base}/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {auth}"
Traceloop.init(app_name="app4-openllmetry", disable_batch=True,
               api_endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"),
               headers={"Authorization": f"Basic {auth}"})

chain1 = prompt1 | model | StrOutputParser()
chain2 = {"city": chain1, "language": itemgetter("language")} | prompt2 | model | StrOutputParser()
answer = chain2.invoke({"person": person, "language": "æ—¥æœ¬èª"}, config={"run_name": "app4-two-step"})
```

## ãŠã‚ã‚Šã«

- LangChain ã¯ä¾å­˜ã‚µãƒ¼ãƒ“ã‚¹ãŒå¤šãã€æ€ã£ãŸã‚ˆã‚Šã‚„ã‚„ã“ã—ã„ã€‚
- ClickHouse ã‚’åˆä½¿ç”¨ï¼ˆè£å´ã§å‹•ã„ã¦ã‚‹ã ã‘ã ã‹ã‚‰ä½•ã‚‚ã—ã¦ãªã„ã‘ã©ï¼‰ã€‚å…¬å¼ã„ã‚ã â€œClickHouse is the fastest and most resource efficient real-time data warehouse and open-source database.â€
- OpenLLMetry ã§ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ OTLP é€ä¿¡ã§ããŸã®ãŒã‚ˆã‹ã£ãŸã€‚ç’°å¢ƒå¤‰æ•°ã®æº–å‚™ã ã‘ã§é€ã‚Œã‚‹ã€‚åŒã˜ã‚³ãƒ¼ãƒ‰ã§ APM (Application Performance Monitoring)ã¨ Langfuse ã®ä¸¡æ–¹ã«é€ã‚Œãã†ã€‚
- OpenLLMetry/Traceloop ã®åˆ¶ç´„ã‚„ Langfuse å´ã®å—ä¿¡ã¾ã‚ã‚Šã¯ã€ã‚‚ã†å°‘ã—æ˜ã‚ŠãŸã„ã€‚

ä»¥ä¸Šï¼
