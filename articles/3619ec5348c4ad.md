---
title: "OpenTelemetry MCP Server動かしてみた"
emoji: "🔭"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [opentelemetry, mcp, grafana, tempo]
published: true
---

## はじめに

OpenTelemetry MCP Server を見つけたので試してみました。

https://github.com/traceloop/opentelemetry-mcp-server

## Codex の設定

最近、Codex を使ってるので Codex で試してみます。

```toml:config.toml
[mcp_servers.opentelemetry]
command = "uvx"
args = [
    "opentelemetry-mcp",
    "--backend",
    "tempo",
    "--url",
    "http://localhost:3200",
]
```

## Tempo の設定

OpenTelemetry の検証は、Docker compose で試してます。以下参照です。

https://zenn.dev/cepe_jp/articles/c07d75daba4e35

以下のように、tempo のポートを開けておきます。
ここでは grafana/otel-lgtm イメージを利用しています。

https://hub.docker.com/r/grafana/otel-lgtm

```yaml
lgtm:
  image: grafana/otel-lgtm:latest
  ports:
    - "3000:3000" # Grafana UI
    - "4317:4317" # OTLP gRPC
    - "4318:4318" # OTLP HTTP
    - "3200:3200" # Tempo
  environment:
    - GF_SECURITY_ADMIN_PASSWORD=admin
  volumes:
    - ./grafana-dashboard-todo.json:/otel-lgtm/grafana/conf/provisioning/dashboards/todo.json:ro
    - ./grafana/provisioning/dashboards/todo.yaml:/otel-lgtm/grafana/conf/provisioning/dashboards/todo.yaml:ro
  networks:
    - observability
```

## 試した結果

こんなやりとりできました。
トレースを見てもらって、改善活動できそうです。

```
ユーザー: 時間がかかってるトレースを探ってほしい
アシスタント: 今の30分・`duration >= 200ms` ではヒットなしでした。
もう少し広げる/下げるならどれがいい？
1) 直近2時間で `>= 200ms`
2) 直近30分で `>= 50ms`
3) 期間指定を教えてください

ユーザー: 30minで>40msくらい
アシスタント: 30分・>40ms の範囲で 20 件ヒットしました。上位の傾向だけ抜き出すと、Spring と Node の `GET /api/v1/todos` が多く、`DELETE` が 40〜86ms くらいで混ざっています。
代表例（最長付近）
- `todo-api` `DELETE /api/v1/todos/{todo_id}` 86.5ms → trace `d6b57f6b5dcec3fce726095b77a226d4`
- `todo-api` `DELETE /api/v1/todos/{todo_id}` 86.1ms → trace `edc2536f68e044ab7146b1510c824930`
- `todo-api-node` `GET /api/v1/todos` 71.9ms → trace `83baf06f52e29c2dc876ba3202203d31`
- `todo-api-spring` `GET /api/v1/todos` 63.6ms → trace `c44611f935603fea4058550227894000`
どれか深掘りする？`get_trace` で詳細見るなら trace_id を指定して。

ユーザー: deleteのやつ。なんで遅めなんやろう？
アシスタント: どの DELETE を掘る？一番長い `d6b57f6b5dcec3fce726095b77a226d4` で `get_trace` して原因見ようか。

ユーザー: お願い
アシスタント: 原因はDBの `DELETE` が占めています。
trace `d6b57f6b5dcec3fce726095b77a226d4` では、全体 86.5ms のうち `DELETE FROM todos` が 81.24ms。HTTP処理やSELECTはほぼ無視できるレベルです。
つまり「遅め」なのはアプリ側よりDBの削除がボトルネック。
次に見るなら、このDELETEが遅い理由（ロック待ち、インデックス、同時更新）を疑うのが筋です。
要るなら、同じテーブルの他の DELETE トレースをまとめて比較しますか？

ユーザー: お願い
アシスタント: DELETE 系はちゃんと取れています。
ざっと見ると「todo-api（FastAPI）」はほとんど 2〜8msで、たまに 20ms台が混じる感じ。
「todo-api-spring」は 2〜11msぐらいで安定。
さっきの「遅め 86ms」のような外れ値は少数派で、DB DELETE が長引く個体だけが目立つ構図です。
次は、遅い個体だけ集めて DB span を比較する？それとも `todo-api` の DELETE の p95/p99 を出して傾向見る？

ユーザー: p95/p99いいね、お願い
アシスタント: 30分・DELETE の p95/p99 を計算しました（それぞれ 50件のサンプル、nearest‑rank法）。
- `todo-api` `DELETE /api/v1/todos/{todo_id}`
  - p95: 約 `8.263ms`
  - p99: 約 `26.108ms`
- `todo-api-spring` `DELETE /api/v1/todos/{id}`
  - p95: 約 `36.938ms`
  - p99: 約 `42.822ms`
注意: どちらも直近30分の50件だけなので、外れ値の影響が強めです。もう少し母数を増やすなら期間を広げて再計算します。
```

## おわりに

最初、設定とかめんどくさいのかなと思いましたが、すんなり動かせました。認証が出てくるともう少し考えないといけないのかもですが、今回はローカルで試して楽チンでした。OpenTelemetry のエコシステムかなり充実してきてるんだろうなと思います。自分もそのうち貢献できればと思います。
