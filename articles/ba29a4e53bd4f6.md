---
title: "Google Colabã§llama-cpp-python"
emoji: "ğŸ¦™"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: [llama, llm, googlecolaboratory]
published: true
---

# ã¯ã˜ã‚ã«

Google Colab ã§ llama-cpp-python ä½¿ã†ã¨ãã«ã€ãƒãƒã£ãŸã“ã¨ãƒ¡ãƒ¢

https://github.com/abetlen/llama-cpp-python

# pip install

`pip install`ã™ã‚‹ã¨ãã€GPU ã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ã€ã“ã“ã‚‰ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦å®Ÿè¡Œã™ã‚‹

```
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python
```

# UnsupportedOperation: fileno

`UnsupportedOperation: fileno`ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€`verbose=True`ã‚’è¨­å®šã™ã‚‹ã€‚
Python ãŒ stdout ã‚„ stderr ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚ˆã†ã¨ã—ãŸã¨ãã«ã€ãã®æ“ä½œãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã«ç™ºç”Ÿã™ã‚‹ã‚‰ã—ã„ã€‚
ä¾‹ãˆã°ã“ã‚“ãªæ„Ÿã˜ã€‚

```py
        chat_handler = Llava15ChatHandler(clip_model_path=clip_model_path, verbose=True)
        self.llm = Llama(
            model_path = model_path,
            chat_format = "llava-1-5",
            chat_handler = chat_handler,
            n_ctx = 2048,
            n_gpu_layers = 1,
            logits_all = True,
            verbose = True
        )
```

# å‚è€ƒ

https://github.com/ggerganov/llama.cpp/issues/128
https://github.com/abetlen/llama-cpp-python/issues/729
https://github.com/guidance-ai/guidance/issues/545
